---
title: "DSC161 Midterm"
output: pdf_document
---
```{r}
library(tidyverse)
library(tokenizers)
library(quanteda)
library(quanteda.textplots)

metadata <- read_csv("UNGDspeeches.csv")
metadata
```


```{r}
corpus_sotu <- corpus(metadata, text_field = "text")
toks <- corpus_sotu %>%
  tokens()
dfm <- dfm(toks)
dfm
```

```{r}
toks <- tokens(corpus_sotu, remove_punct = TRUE, remove_numbers=TRUE)
toks <- tokens_wordstem(toks)
toks <- tokens_select(toks,  stopwords("en"), selection = "remove")
dfm <- dfm(toks)
dfm
```
```{r}
dfm_trimmed <- dfm_trim(dfm, min_docfreq = 0.05, docfreq_type = "prop")
dfm_trimmed
#2,471 features
```
```{r}
textplot_wordcloud(dfm_trimmed, col="black")
```
```{r}
#Subset dfm and meatadata to speech made by the US and Canada
dfm_trimmed <- dfm_trimmed[metadata$country%in%c("PSE", "ISR"),]
metadata <- metadata[metadata$country%in%c("PSE", "ISR"),]
```

```{r}
#A different perspective: Fightin' words
clusterFightinWords <- function(dfm, clust.vect, alpha.0=100) {
  # we need to get the overall corpus word distribution and the cluster-specific words dists
  # y_{kw} in Monroe et al. 
  overall.terms <- colSums(dfm)
  # n and n_k in Monroe et al. 
  n <- sum(overall.terms)
  # alpha_{kw} in Monroe et al. 
  prior.terms <- overall.terms / n * alpha.0
  # y_{kw}(i) in Monroe et al.
  cluster.terms <- colSums(dfm[clust.vect, ])
  # n_k(i) in Monroe et al.
  cluster.n <- sum(cluster.terms)
  
  cluster.term.odds <- 
    (cluster.terms + prior.terms) / 
    (cluster.n + alpha.0 - cluster.terms - prior.terms)
  overall.term.odds <- 
    (overall.terms + prior.terms) / 
    (n + alpha.0 - overall.terms - prior.terms)
```


```{r}
log.odds <- log(cluster.term.odds) - log(overall.term.odds)
  
  variance <- 1/(cluster.terms + prior.terms) + 1/(overall.terms + prior.terms)
  
  # return the variance weighted log-odds for each term
  output <- log.odds / sqrt(variance)
  names(output) <- colnames(dfm)
  return(output)
}
```

```{r}
#Find words that are distinctive of Canada

#terms <- clusterFightinWords(dfm_trimmed, metadata$country=="ISR")
#sort(terms, decreasing=T)[1:10]

terms <- clusterFightinWords(dfm_trimmed, 
                             (metadata$country=="ISR") & (metadata$year < 1998))
sort(terms, decreasing=T)[1:20]
```
```{r}
terms <- clusterFightinWords(dfm_trimmed, 
                             (metadata$country=="ISR") & (metadata$year >= 1998))
sort(terms, decreasing=T)[1:20]
```

```{r}
#Find words that are distinctive of US

terms <- clusterFightinWords(dfm_trimmed, 
                             metadata$country=="PSE")
sort(terms, decreasing=T)[1:20]

```
```{r}
#install.packages("stm")
library(stm)
#install.packages("seededlda")
library(seededlda)
```

```{r}
#STM
#Process the data to put it in STM format.  Textprocessor automatically does preprocessing
temp <- textProcessor(documents=metadata$text,metadata=metadata)
#prepDocuments removes words/docs that are now empty after preprocessing
out <- prepDocuments(temp$documents, temp$vocab, temp$meta)
```
```{r}
#Let's try to distinguish between topics that are spoken/written and by year

#This takes a bit. You'd want to remove max.em.its -- this is just to make it shorter!
#Here we are using prevalence covariate sotu_type and year
model.stm <- stm(out$documents, out$vocab, K = 10, prevalence = ~country + s(year),
                 data = out$meta, max.em.its = 10) 
#Find most probable words in each topic
labelTopics(model.stm)
```
```{r}
#And most common topics
plot(model.stm, n=10)
```
```{r}
#Get representative documents
#findThoughts(model.stm, texts=out$meta$year, topics=3, n=20)
findThoughts(model.stm, texts=out$meta$country, topics=3, n=10)
#findThoughts(model.stm, texts=out$meta$text, topics=10, n=10)
```

```{r}
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
plot(model.stm.ee, "year", method="continuous", topics=2)
```

