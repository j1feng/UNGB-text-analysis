# y_{kw} in Monroe et al.
overall.terms <- colSums(dfm)
# n and n_k in Monroe et al.
n <- sum(overall.terms)
# alpha_{kw} in Monroe et al.
prior.terms <- overall.terms / n * alpha.0
# y_{kw}(i) in Monroe et al.
cluster.terms <- colSums(dfm[clust.vect, ])
# n_k(i) in Monroe et al.
cluster.n <- sum(cluster.terms)
cluster.term.odds <-
(cluster.terms + prior.terms) /
(cluster.n + alpha.0 - cluster.terms - prior.terms)
overall.term.odds <-
(overall.terms + prior.terms) /
(n + alpha.0 - overall.terms - prior.terms)
log.odds <- log(cluster.term.odds) - log(overall.term.odds)
variance <- 1/(cluster.terms + prior.terms) + 1/(overall.terms + prior.terms)
# return the variance weighted log-odds for each term
output <- log.odds / sqrt(variance)
names(output) <- colnames(dfm)
return(output)
}
#Find words that are distinctive of US
terms <- clusterFightinWords(dfm_trimmed,
metadata$country=="USA")
sort(terms, decreasing=T)[1:10]
#Find words that are distinctive of US
terms <- clusterFightinWords(dfm_trimmed,
metadata$country=="CAN")
sort(terms, decreasing=T)[1:10]
#Find words that are distinctive of US
terms <- clusterFightinWords(dfm_trimmed,
metadata$country=="USA")
sort(terms, decreasing=T)[1:10]
#Find words that are distinctive of Canada
terms <- clusterFightinWords(dfm_trimmed,
metadata$country=="CAN")
sort(terms, decreasing=T)[1:10]
#Find words that are distinctive of US
terms <- clusterFightinWords(dfm_trimmed,
metadata$country=="USA")
sort(terms, decreasing=T)[1:10]
install.packages("stm")
#library(stm)
install.packages("seedelda")
#(seededlda)
#install.packages("stm")
library(stm)
#install.packages("seedelda")
(seededlda)
#install.packages("stm")
library(stm)
#install.packages("seedelda")
library(seededlda)
#install.packages("stm")
library(stm)
install.packages("seedelda")
library(seededlda)
#install.packages("stm")
library(stm)
install.packages("seedelda")
library(seededlda)
#install.packages("stm")
library(stm)
install.packages("seedelda")
library(seededlda)
#install.packages("stm")
library(stm)
install.packages("seededlda")
library(seededlda)
#install.packages("stm")
library(stm)
#install.packages("seededlda")
library(seededlda)
metadata
#STM
#Process the data to put it in STM format.  Textprocessor automatically does preprocessing
temp<-textProcessor(documents=metadata$text,metadata=metadata)
#STM
#Process the data to put it in STM format.  Textprocessor automatically does preprocessing
temp<-textProcessor(documents=metadata$text,metadata=metadata)
#STM
#Process the data to put it in STM format.  Textprocessor automatically does preprocessing
temp<-textProcessor(documents=metadata$text,metadata=metadata)
#install.packages("stm")
library(stm)
#install.packages("seededlda")
library(seededlda)
#STM
#Process the data to put it in STM format.  Textprocessor automatically does preprocessing
temp<-textProcessor(documents=metadata$text,metadata=metadata)
#STM
#Process the data to put it in STM format.  Textprocessor automatically does preprocessing
temp <- textProcessor(documents=metadata$text,metadata=metadata)
#STM
#Process the data to put it in STM format.  Textprocessor automatically does preprocessing
temp <- textProcessor(documents=metadata$text,metadata=metadata)
install.packages("tm")
#STM
#Process the data to put it in STM format.  Textprocessor automatically does preprocessing
temp <- textProcessor(documents=metadata$text,metadata=metadata)
#prepDocuments removes words/docs that are now empty after preprocessing
out <- prepDocuments(temp$documents, temp$vocab, temp$meta)
#Let's try to distinguish between topics that are spoken/written and by year
#This takes a bit. You'd want to remove max.em.its -- this is just to make it shorter!
#Here we are using prevalence covariate sotu_type and year
model.stm <- stm(out$documents, out$vocab, K = 10, prevalence = ~country + s(year),
data = out$meta, max.em.its = 10)
#Find most probable words in each topic
labelTopics(model.stm)
#And most common topics
plot(model.stm, n=10)
#Get representative documents
findThoughts(model.stm, texts=out$meta$text, topics=8, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$text, topics=1, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$country, topics=1, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$text, topics=1, n=3)
metadata
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=1, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=1, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=1, n=3)
findThoughts(model.stm, texts=out$meta$country, topics=1, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=1, n=3)
findThoughts(model.stm, texts=out$meta$country, topics=1, n=3)
findThoughts(model.stm, texts=out$meta$text, topics=1, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=2, n=3)
findThoughts(model.stm, texts=out$meta$country, topics=2, n=3)
findThoughts(model.stm, texts=out$meta$text, topics=2, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=2, n=5)
findThoughts(model.stm, texts=out$meta$country, topics=2, n=3)
findThoughts(model.stm, texts=out$meta$text, topics=2, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=2, n=3)
findThoughts(model.stm, texts=out$meta$country, topics=2, n=3)
findThoughts(model.stm, texts=out$meta$text, topics=2, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=3, n=3)
findThoughts(model.stm, texts=out$meta$country, topics=3, n=3)
findThoughts(model.stm, texts=out$meta$text, topics=3, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=4, n=3)
findThoughts(model.stm, texts=out$meta$country, topics=4, n=3)
findThoughts(model.stm, texts=out$meta$text, topics=4, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=5, n=3)
findThoughts(model.stm, texts=out$meta$country, topics=5, n=3)
findThoughts(model.stm, texts=out$meta$text, topics=5, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=5, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=5, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=5, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=1, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=1, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=1, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=6, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=6, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=6, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=7, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=7, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=7, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=7, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=7, n=10)
#findThoughts(model.stm, texts=out$meta$text, topics=7, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=7, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=7, n=10)
#findThoughts(model.stm, texts=out$meta$text, topics=7, n=3)
#Get representative documents
#findThoughts(model.stm, texts=out$meta$year, topics=7, n=10)
#findThoughts(model.stm, texts=out$meta$country, topics=7, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=7, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=8, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=8, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=8, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=9, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=9, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=9, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=9, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=9, n=20)
findThoughts(model.stm, texts=out$meta$text, topics=9, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=9, n=20)
findThoughts(model.stm, texts=out$meta$country, topics=9, n=20)
findThoughts(model.stm, texts=out$meta$text, topics=9, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=10, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=10, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=10, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=10, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=10, n=10)
#findThoughts(model.stm, texts=out$meta$text, topics=10, n=3)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=10, n=10)
findThoughts(model.stm, texts=out$meta$country, topics=10, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=10, n=3)
#Get representative documents
#findThoughts(model.stm, texts=out$meta$year, topics=10, n=10)
#findThoughts(model.stm, texts=out$meta$country, topics=10, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=10, n=3)
#Get representative documents
#findThoughts(model.stm, texts=out$meta$year, topics=10, n=10)
#findThoughts(model.stm, texts=out$meta$country, topics=10, n=10)
findThoughts(model.stm, texts=out$meta$text, topics=10, n=10)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=10, n=20)
findThoughts(model.stm, texts=out$meta$country, topics=10, n=20)
#findThoughts(model.stm, texts=out$meta$text, topics=10, n=10)
#Get representative documents
#findThoughts(model.stm, texts=out$meta$year, topics=10, n=20)
#findThoughts(model.stm, texts=out$meta$country, topics=10, n=20)
findThoughts(model.stm, texts=out$meta$text, topics=10, n=0)
#Get representative documents
#findThoughts(model.stm, texts=out$meta$year, topics=10, n=20)
#findThoughts(model.stm, texts=out$meta$country, topics=10, n=20)
findThoughts(model.stm, texts=out$meta$text, topics=10, n=10)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=10, n=20)
#findThoughts(model.stm, texts=out$meta$country, topics=10, n=20)
#findThoughts(model.stm, texts=out$meta$text, topics=10, n=10)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=10, n=20)
#findThoughts(model.stm, texts=out$meta$country, topics=10, n=20)
findThoughts(model.stm, texts=out$meta$text, topics=10, n=10)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=3, n=20)
findThoughts(model.stm, texts=out$meta$country, topics=3, n=20)
#findThoughts(model.stm, texts=out$meta$text, topics=10, n=10)
#Get representative documents
findThoughts(model.stm, texts=out$meta$year, topics=3, n=20)
findThoughts(model.stm, texts=out$meta$country, topics=3, n=20)
findThoughts(model.stm, texts=out$meta$text, topics=10, n=10)
#Get representative documents
#findThoughts(model.stm, texts=out$meta$year, topics=3, n=20)
#findThoughts(model.stm, texts=out$meta$country, topics=3, n=20)
#findThoughts(model.stm, texts=out$meta$text, topics=10, n=10)
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
plot(model.stm.ee, "country")
#plot(model.stm.ee, "sotu_type", method="difference", cov.value1="speech", cov.value2="written")
#plot(model.stm.ee, "year", method="continuous", topics=8)
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
#plot(model.stm.ee, "country")
#plot(model.stm.ee, "sotu_type", method="difference", cov.value1="speech", cov.value2="written")
plot(model.stm.ee, "year", method="continuous", topics=8)
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
#plot(model.stm.ee, "country")
#plot(model.stm.ee, "sotu_type", method="difference", cov.value1="speech", cov.value2="written")
plot(model.stm.ee, "country", "year", method="continuous", topics=8)
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
plot(model.stm.ee, "country", method="continuous", topics=8)
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
plot(model.stm.ee, "country", method="continuous", topics=8)
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
plot(model.stm.ee, "year", method="continuous", topics=8)
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
plot(model.stm.ee, "country", method="continuous", topics=8)
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
plot(model.stm.ee, "year", method="continuous", topics=8)
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
plot(model.stm.ee, "year", method="continuous")
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
plot(model.stm.ee, "year", method="continuous", topics=1)
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
plot(model.stm.ee, "year", method="continuous", topics=2)
model.stm.ee <- estimateEffect(1:10 ~ country + s(year), model.stm, meta = out$meta)
plot(model.stm.ee, "year", method="continuous", topics=2)
#loading the packages
library(tidyverse)
library(tokenizers)
library(quanteda)
library(quanteda.textplots)
#install.packages("stm")
library(stm)
#install.packages("seededlda")
library(seededlda)
metadata <- read_csv("UNGDspeeches.csv")
head(metadata)
for (i in 1:nrow(metadata)){
#if the country is israel
if (metadata[i, ]$country == 'ISR'){
if (metadata[i, ]$year < 1998) {
metadata[i, 'country'] = 'ISR_prev_1998'
} else {
metadata[i, 'country'] = 'ISR_post_1998'
}
}
}
#use quanteda to turn the data into a corpus
corpus_un <- corpus(metadata, text_field = "text")
toks_un <- tokens(corpus_un)
dfm_un <- dfm(toks_un)
dfm_un
#remove punct, stopwords.. etc
toks_un <- tokens(corpus_un, remove_punct = TRUE, remove_numbers=TRUE)
toks_un <- tokens_wordstem(toks_un)
toks_un <- tokens_select(toks_un,  stopwords("en"), selection = "remove")
dfm_un <- dfm(toks_un)
dfm_un
dfm_trimmed <- dfm_trim(dfm_un, min_docfreq = 0.05, docfreq_type = "prop")
dfm_trimmed
#2,471 features
#all word based on their word frequency.
textplot_wordcloud(dfm_trimmed, col="black")
#Subset dfm and metadata to speech made by the Israel before 1998, after 1998, and Palestine.
dfm_trimmed <- dfm_trimmed[metadata$country%in%c("PSE", "ISR_prev_1998", "ISR_post_1998"),]
metadata <- metadata[metadata$country%in%c("PSE", "ISR_prev_1998", "ISR_post_1998"),]
textplot_wordcloud(dfm_trimmed[metadata$country == "PSE",], col="darkgreen")
textplot_wordcloud(dfm_trimmed[metadata$country%in%c("ISR_prev_1998", "ISR_post_1998"),])
textplot_wordcloud(dfm_trimmed[metadata$country=="ISR_prev_1998",],
col = 'deepskyblue4')
textplot_wordcloud(dfm_trimmed[metadata$country=="ISR_post_1998",],
col = 'dodgerblue4')
#DSC161 codes: Fightin' words
clusterFightinWords <- function(dfm, clust.vect, alpha.0=100) {
# we need to get the overall corpus word distribution and the cluster-specific words dists
# y_{kw} in Monroe et al.
overall.terms <- colSums(dfm)
# n and n_k in Monroe et al.
n <- sum(overall.terms)
# alpha_{kw} in Monroe et al.
prior.terms <- overall.terms / n * alpha.0
# y_{kw}(i) in Monroe et al.
cluster.terms <- colSums(dfm[clust.vect, ])
# n_k(i) in Monroe et al.
cluster.n <- sum(cluster.terms)
cluster.term.odds <-
(cluster.terms + prior.terms) /
(cluster.n + alpha.0 - cluster.terms - prior.terms)
overall.term.odds <-
(overall.terms + prior.terms) /
(n + alpha.0 - overall.terms - prior.terms)
log.odds <- log(cluster.term.odds) - log(overall.term.odds)
variance <- 1/(cluster.terms + prior.terms) + 1/(overall.terms + prior.terms)
# return the variance weighted log-odds for each term
output <- log.odds / sqrt(variance)
names(output) <- colnames(dfm)
return(output)
}
terms <- clusterFightinWords(dfm_trimmed,
metadata$country=="ISR_post_1998")
sort(terms, decreasing=T)[1:10]
#Find words that are distinctive of Israel before 1998, after 1998, and Palestine
#terms <- clusterFightinWords(dfm_trimmed, metadata$country=="ISR")
#sort(terms, decreasing=T)[1:10]
terms <- clusterFightinWords(dfm_trimmed,
metadata$country=="ISR_prev_1998")
sort(terms, decreasing=T)[1:10]
#Find words that are distinctive of PSE
terms <- clusterFightinWords(dfm_trimmed,
metadata$country=="PSE")
sort(terms, decreasing=T)[1:10]
dfm_trimmed
#dfm_trimmed
#LDA
######
#Run LDA using quanteda
lda <- textmodel_lda(dfm_trimmed, k = 10)
#Most likely term for each topic
lda.terms <- terms(lda, 10)
lda.terms
#Topical content matrix
mu <- lda$phi
dim(mu) #10 topics, 5923 words
mu[1:10,1:20]
#Most representative words in Topic 1
mu[1,][order(mu[1,], decreasing=T)][1:10]
#Topical prevalence matrix
pi <- lda$theta
dim(pi) #number of docs by number of topics
#Most representative documents in Topic 1
metadata[order(pi[1,],decreasing=T),]
#LDA
######
#Run LDA using quanteda
lda <- textmodel_lda(dfm_trimmed, k = 5)
#Most likely term for each topic
lda.terms <- terms(lda, 5)
lda.terms
#Topical content matrix
mu <- lda$phi
dim(mu) #10 topics, 5923 words
mu[1:10,1:20]
dim(mu)
dim(mu)
dim(mu)
#LDA
######
#Run LDA using quanteda
lda <- textmodel_lda(dfm_trimmed, k = 5)
#Most likely term for each topic
lda.terms <- terms(lda, 5)
lda.terms
#Topical content matrix
mu <- lda$phi
dim(mu) #5 topics, 2741 words
mu[1:5,1:10]
#Most representative words in Topic 1
mu[1,][order(mu[1,], decreasing=T)][1:10]
#Topical prevalence matrix
pi <- lda$theta
dim(pi) #number of docs by number of topics
#Most representative documents in Topic 1
metadata[order(pi[1,],decreasing=T),]
#STM
#Process the data to put it in STM format.Textprocessor() automatically does pre-processing
temp <- textProcessor(documents=metadata$text,metadata=metadata)
#prepDocuments() removes words/docs that are now empty after pre-processing
out <- prepDocuments(temp$documents, temp$vocab, temp$meta)
isrmeta = metadata[metadata$country != "PSE", ] # Subset, only include Israeli documents
isrtemp = textProcessor(documents = isrmeta$text, metadata = isrmeta) # Preprocessing
isrout <- prepDocuments(isrtemp$documents, isrtemp$vocab, isrtemp$meta)
isrmode <- stm(isrout$documents, isrout$vocab, K = 2,
prevalence = ~s(year), data = isrout$meta) # Run STM
labelTopics(isrmode) # Interprete results
isrmode.ee <- estimateEffect(1:2 ~ s(year), isrmode, meta = isrout$meta) # Estimate Effect
plot(isrmode.ee, "year", method = "continuous", topics = 2) # Plot effect
abline(v = 1998)
# text(locator(), labels = c("1998")) # Requires interaction
#STM
#Process the data to put it in STM format.Textprocessor() automatically does pre-processing
temp <- textProcessor(documents=metadata$text,metadata=metadata)
#prepDocuments() removes words/docs that are now empty after pre-processing
out <- prepDocuments(temp$documents, temp$vocab, temp$meta)
#Let's try to distinguish between topics
#number of topic
num_topic = 5
model.stm <- stm(out$documents, out$vocab, K = num_topic, prevalence = ~country + s(year),
data = out$meta)
#Let's try to distinguish between topics
#number of topic
num_topic = 5
model.stm <- stm(out$documents, out$vocab, K = num_topic, prevalence = ~country + s(year),
data = out$meta)
#Find most probable words in each topic
labelTopics(model.stm)
#And most common topics
plot(model.stm)
topic_words =
c("palestinian, peac, peopl, state, intern, will, israel",
"israel, iran, will, peac, year, peopl, nation",
"peac, peopl, will, nation, new, can, palestinian",
"israel, arab, peac, state, nation, unit, agreement",
"israel, peac, nation, will, unit, state, countri"
)
model.stm.ee <- estimateEffect(1:num_topic ~ country + s(year), model.stm, meta = out$meta)
dev.new(width=100, height=50, unit="in")
plot(model.stm.ee, "country", main="Topic num vs. Countries")
for (i in 1:num_topic){
#plot(model.stm.ee, "country")
plot(model.stm.ee, "year", method="continuous", topics=i, main = paste("Topic ", i, ": ", topic_words[i]))
}
findThoughts(model.stm, texts=out$meta$year, topics=1, n=3)$docs
findThoughts(model.stm, texts=out$meta$country, topics=1, n=3)$docs
df = data.frame(matrix(vector(), 0, 4,
dimnames=list(c(), c("Topic", "Year", "Country", "Text"))),
stringsAsFactors=T)
for (i in 1:num_topic){
df[(i-1) * 10 + 1: (i * 10), "Topic"] = list(rep(i,10))
df[(i-1) * 10 + 1: (i * 10), "Word"] = topic_words[i]
df[(i-1) * 10 + 1: (i * 10), "Year"] = findThoughts(model.stm, texts=out$meta$year, topics=i, n=10)$docs
df[(i-1) * 10 + 1: (i * 10), "Country"] = findThoughts(model.stm, texts=out$meta$country, topics=i, n=10)$docs
df[(i-1) * 10 + 1: (i * 10), "Text"] = findThoughts(model.stm, texts=out$meta$text, topics=i, n=10)$docs
}
head(df, 2)
df = data.frame(matrix(vector(), 0, 4,
dimnames=list(c(), c("Topic", "Word", "Year", "Country", "Text"))),
stringsAsFactors=T)
df = data.frame(matrix(vector(), 0, 5,
dimnames=list(c(), c("Topic", "Word", "Year", "Country", "Text"))),
stringsAsFactors=T)
for (i in 1:num_topic){
df[(i-1) * 10 + 1: (i * 10), "Topic"] = list(rep(i,10))
df[(i-1) * 10 + 1: (i * 10), "Word"] = topic_words[i]
df[(i-1) * 10 + 1: (i * 10), "Year"] = findThoughts(model.stm, texts=out$meta$year, topics=i, n=10)$docs
df[(i-1) * 10 + 1: (i * 10), "Country"] = findThoughts(model.stm, texts=out$meta$country, topics=i, n=10)$docs
df[(i-1) * 10 + 1: (i * 10), "Text"] = findThoughts(model.stm, texts=out$meta$text, topics=i, n=10)$docs
}
head(df, 2)
write.csv(df,'topic_model_output.csv', row.names = FALSE)
